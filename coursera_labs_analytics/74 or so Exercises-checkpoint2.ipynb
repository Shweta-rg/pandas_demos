{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10746d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc4bd4f0",
   "metadata": {},
   "source": [
    "1) Demonstrate how to import Pandas and check the version of the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a3bc97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.2'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc4f4a",
   "metadata": {},
   "source": [
    "2) Transform this list, numpy array, and dictionary into a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "21b30b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mylist = list('abcedfghijklmnopqrstuvwxyz')\n",
    "s1=pd.Series(mylist,name='mylist')\n",
    "#print(s1)\n",
    "\n",
    "myarr = np.arange(26)\n",
    "#print(myarr)\n",
    "mydict = dict(zip(mylist, myarr))\n",
    "#print(mydict)\n",
    "dict_ser=pd.Series(mydict)\n",
    "#print(dict_ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17155a8",
   "metadata": {},
   "source": [
    "3) Convert the index of your previous series into a column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "db0f1395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0\n",
      "a    0\n",
      "b    1\n",
      "c    2\n",
      "e    3\n",
      "d    4\n",
      "..  ..\n",
      "v   21\n",
      "w   22\n",
      "x   23\n",
      "y   24\n",
      "z   25\n",
      "\n",
      "[26 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "ser_frame=dict_ser.to_frame()\n",
    "print(ser_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f625f24",
   "metadata": {},
   "source": [
    "4) Combine the series below into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d268747c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1\n",
      "0   a   0\n",
      "1   b   1\n",
      "2   c   2\n",
      "3   e   3\n",
      "4   d   4\n",
      ".. ..  ..\n",
      "21  v  21\n",
      "22  w  22\n",
      "23  x  23\n",
      "24  y  24\n",
      "25  z  25\n",
      "\n",
      "[26 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ser1 = pd.Series(list('abcedfghijklmnopqrstuvwxyz'))\n",
    "ser2 = pd.Series(np.arange(26))\n",
    "newdataframe=pd.concat([ser1,ser2],axis=1)\n",
    "print(newdataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdd8618",
   "metadata": {},
   "source": [
    "5) Give the series below a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b18f9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "ser = pd.Series(list('abcedfghijklmnopqrstuvwxyz'),name='Alphabets')\n",
    "#print(ser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc8d244",
   "metadata": {},
   "source": [
    "6) Find the elements in the first series (ser1) not in the second series (ser2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f1276978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "ser1 = pd.Series([1, 2, 3, 4, 5])\n",
    "ser2 = pd.Series([4, 5, 6, 7, 8])\n",
    "ser1[~(ser1.isin(ser2))]\n",
    "print(ser1[~(ser1.isin(ser2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "00c92e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    }
   ],
   "source": [
    "ser1 = pd.Series([1, 2, 3, 4, 5])\n",
    "ser2 = pd.Series([4, 5, 6, 7, 8])\n",
    "ser3=[]\n",
    "for i in ser1:\n",
    "    if i not in ser2:\n",
    "        ser3.append(i)\n",
    "print(ser3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf31f60f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d336f7dd",
   "metadata": {},
   "source": [
    "7) Find the elements in both series that are not in common (remove them if they exist in both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "28768232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "2    6\n",
       "3    7\n",
       "4    8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser1 = pd.Series([1, 2, 3, 4, 5])\n",
    "ser2 = pd.Series([4, 5, 6, 7, 8])\n",
    "combine = pd.concat([ser1, ser2])\n",
    "combine\n",
    "common_list = []\n",
    "common = ser1[ser1.isin(ser2)]\n",
    "common\n",
    "#ans = ser1[~ser1.isin(ser2)]\n",
    "ans = combine[~combine.isin(common)]\n",
    "ans\n",
    "# ser_u = pd.Series(np.union1d(ser1, ser2))  # union\n",
    "# ser_i = pd.Series(np.intersect1d(ser1, ser2))  # intersect\n",
    "# ser_u[~ser_u.isin(ser_i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7188b5",
   "metadata": {},
   "source": [
    "8) Find the minimum, max, 25th percentile, and 75th percentile in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5a1a3ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0\n",
      "count  25.000000\n",
      "mean   10.661614\n",
      "std     4.285653\n",
      "min    -0.102328\n",
      "25%     8.271421\n",
      "50%    11.638017\n",
      "75%    14.323931\n",
      "max    15.852063\n"
     ]
    }
   ],
   "source": [
    "ser = pd.Series(np.random.normal(10, 5, 25))\n",
    "df=ser.to_frame()\n",
    "df1=df.describe(include='all')\n",
    "print(df1)\n",
    "#using numpy we got the array\n",
    "# np.percentile(ser, q=[0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c1803",
   "metadata": {},
   "source": [
    "9) Obtain the count of each unique item in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16a2880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f    7\n",
       "b    6\n",
       "h    5\n",
       "c    4\n",
       "d    3\n",
       "a    2\n",
       "e    2\n",
       "g    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(np.take(list('abcdefgh'), np.random.randint(8, size=30)))\n",
    "ser.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449036ea",
   "metadata": {},
   "source": [
    "10) Keep the two most frequent items in the series and change all items that are not those two into \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e86f2a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4    5\n",
      "2    4\n",
      "3    2\n",
      "1    1\n",
      "dtype: int64\n",
      "[4, 2]\n",
      "[3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_16204\\97791223.py:7: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  list1=ser.value_counts()[:2].index.tolist()\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_16204\\97791223.py:9: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  list2=ser.value_counts()[2:].index.tolist()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         4\n",
       "1         2\n",
       "2         4\n",
       "3         2\n",
       "4     Other\n",
       "      ...  \n",
       "7         4\n",
       "8         2\n",
       "9         2\n",
       "10        4\n",
       "11    Other\n",
       "Length: 12, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(100)\n",
    "ser = pd.Series(np.random.randint(1, 5, [12]))\n",
    "ser2=ser.value_counts()\n",
    "print(ser2)\n",
    "#df=ser.to_frame()\n",
    "#print(df.value_counts())\n",
    "list1=ser.value_counts()[:2].index.tolist()\n",
    "print(list1)\n",
    "list2=ser.value_counts()[2:].index.tolist()\n",
    "print(list2)\n",
    "#list1=df[0].value_counts()[:2].index.tolist()\n",
    "type(list1)\n",
    "\n",
    "ser[~ser.isin(ser.value_counts().index[:2])] = 'Other'\n",
    "ser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46a38688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    1\n",
      "8    1\n",
      "dtype: int32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         4\n",
       "1         4\n",
       "2         3\n",
       "3         3\n",
       "4         3\n",
       "      ...  \n",
       "7         4\n",
       "8     Other\n",
       "9         4\n",
       "10        4\n",
       "11        3\n",
       "Length: 12, dtype: object"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(100)\n",
    "ser = pd.Series(np.random.randint(1, 5, [12]))\n",
    "print(ser[~ser.isin(ser.value_counts().index[:2])])\n",
    "ser[~ser.isin(ser.value_counts().index[:2])] = 'Other'\n",
    "ser\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5928875",
   "metadata": {},
   "source": [
    "11) Bin the series below into 10 equal deciles and replace the values with the bin name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "979b3d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10th\n",
       "1     5th\n",
       "2     5th\n",
       "3     7th\n",
       "4     5th\n",
       "dtype: category\n",
       "Categories (10, object): ['1st' < '2nd' < '3rd' < '4th' ... '7th' < '8th' < '9th' < '10th']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(np.random.random(20))\n",
    "\n",
    "pd.qcut(ser, q=[0, .10, .20, .3, .4, .5, .6, .7, .8, .9, 1], \n",
    "        labels=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']).head()\n",
    "pd.cut(ser, bins=[0, .10, .20, .3, .4, .5, .6, .7, .8, .9, 1], \n",
    "        labels=['1st', '2nd', '3rd', '4th', '5th', '6th', '7th', '8th', '9th', '10th']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee79a87",
   "metadata": {},
   "source": [
    "12) Reshape the series ser into a dataframe with 7 rows and 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "17a3513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  2  3  4\n",
      "0  1  3  2  5  3\n",
      "1  9  3  2  7  7\n",
      "2  3  7  9  3  4\n",
      "3  8  1  4  1  5\n",
      "4  7  3  4  4  2\n",
      "5  2  8  1  8  7\n",
      "6  2  2  6  6  4\n"
     ]
    }
   ],
   "source": [
    "ser = pd.Series(np.random.randint(1, 10, 35))\n",
    "df = pd.DataFrame(ser.values.reshape(7,5))\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c62e92",
   "metadata": {},
   "source": [
    "13) Find the positions of numbers that are multiples of 3 from ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7ae884a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    6\n",
      "1    2\n",
      "2    6\n",
      "3    5\n",
      "4    7\n",
      "5    9\n",
      "6    9\n",
      "dtype: int32\n",
      "Int64Index([0, 2, 5, 6], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "ser = pd.Series(np.random.randint(1, 10, 7))\n",
    "print(ser)\n",
    "print(ser[ser.mod(3)==0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a60f247",
   "metadata": {},
   "source": [
    "14) From ser, extract the items at positions in list pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "42338aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     a\n",
       "4     e\n",
       "8     i\n",
       "14    o\n",
       "20    u\n",
       "dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(list('abcdefghijklmnopqrstuvwxyz'))\n",
    "pos = [0, 4, 8, 14, 20]\n",
    "ser[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da1836c",
   "metadata": {},
   "source": [
    "15) Stack ser1 and ser2 vertically and horizontally (to form a dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5dfea534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>v</td>\n",
       "      <td>v</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>w</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>z</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1\n",
       "0   a  a\n",
       "1   b  b\n",
       "2   c  c\n",
       "3   d  d\n",
       "4   e  e\n",
       ".. .. ..\n",
       "21  v  v\n",
       "22  w  w\n",
       "23  x  x\n",
       "24  y  y\n",
       "25  z  z\n",
       "\n",
       "[26 rows x 2 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser1 = pd.Series(range(5))\n",
    "ser2 = pd.Series(list('abcde'))\n",
    "pd.concat([ser,ser],axis=0)  #datframe as have 1 column\n",
    "pd.concat([ser,ser],axis=1)   #dataframe as have 2 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252d7bb",
   "metadata": {},
   "source": [
    "16) Get the positions of items of ser2 in ser1 as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "da51891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 5, 8]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ser1 = pd.Series([10, 9, 6, 5, 3, 1, 12, 8, 13])\n",
    "ser2 = pd.Series([1, 3, 10, 13])\n",
    "ser1[ser1.isin(ser2)].index.to_list()\n",
    "#ser1(ser2.isin(ser1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e34c8",
   "metadata": {},
   "source": [
    "17) Compute the mean squared error of truth and pred series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "df7ff599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26130948300570783"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "from sklearn.metrics import mean_squared_error\n",
    "truth = pd.Series(range(10))\n",
    "pred = pd.Series(range(10)) + np.random.random(10)\n",
    "mean_squared_error(truth, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf6f79",
   "metadata": {},
   "source": [
    "18) Change the first character of each word to upper case in each word of ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c210bbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     How\n",
      "1      To\n",
      "2    Kick\n",
      "3    Ass?\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "ser = pd.Series(['how', 'to', 'kick', 'ass?'])\n",
    "newSeries = ser.map(lambda x: x[0].upper() + x[1:])\n",
    "print(newSeries)\n",
    "\n",
    "#if we want to convert 1st and last word in uppercase\n",
    "#newSeries = ser.map(lambda x: x[0].upper() + x[1:-1] + x[-1].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f198a",
   "metadata": {},
   "source": [
    "19) Caluculate the number of characters for each element in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "216f9a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    2\n",
       "2    4\n",
       "3    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(['how', 'to', 'kick', 'ass?'])\n",
    "ser.str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bcf59",
   "metadata": {},
   "source": [
    "20) Caluculate the difference of differences between the consequtive numbers of ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c6dba0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NaN\n",
       "1    NaN\n",
       "2    1.0\n",
       "3    1.0\n",
       "4    1.0\n",
       "5    1.0\n",
       "6    0.0\n",
       "7    2.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series([1, 3, 6, 10, 15, 21, 27, 35])\n",
    "ser.diff()\n",
    "ser.diff().diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64a2901",
   "metadata": {},
   "source": [
    "21) Convert the date-strings to a timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db82de89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2010-01-01 00:00:00', '2011-02-02 00:00:00',\n",
       "               '2012-03-03 00:00:00', '2013-04-04 00:00:00',\n",
       "               '2014-05-05 00:00:00', '2015-06-06 12:20:00'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])\n",
    "pd.DatetimeIndex(ser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f83c0",
   "metadata": {},
   "source": [
    "22) Get the day of month, week number, day of year and day of week from ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4bcc124e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method _inherit_from_data.<locals>.method of DatetimeIndex(['2010-01-01 00:00:00', '2011-02-02 00:00:00',\n",
       "               '2012-03-03 00:00:00', '2013-04-04 00:00:00',\n",
       "               '2014-05-05 00:00:00', '2015-06-06 12:20:00'],\n",
       "              dtype='datetime64[ns]', freq=None)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(['01 Jan 2010', '02-02-2011', '20120303', '2013/04/04', '2014-05-05', '2015-06-06T12:20'])\n",
    "df = pd.DatetimeIndex(ser).day_name\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f97b7",
   "metadata": {},
   "source": [
    "23) Change ser to dates that start with 4th of the respective months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a6384074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year  week  day\n",
      "0  2009    53    5\n",
      "1  2011     5    2\n",
      "2  2012     9    4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   2010-01-04\n",
       "1   2011-02-04\n",
       "2   2012-03-04\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ser = pd.Series(['Jan 2010', 'Feb 2011', 'Mar 2012'])\n",
    "print(pd.to_datetime(ser).dt.isocalendar())\n",
    "#ser.dt.isocalendar()  \n",
    "#ser = pd.Series(['Jan 2010', 'Feb 2011', 'Mar 2012'])\n",
    "#ser.map(lambda x : \"04\" +x)\n",
    "pd.to_datetime(ser.map(lambda x : \"04 \" + x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb2f54",
   "metadata": {},
   "source": [
    "24) From ser, extract words that contain at least 2 vowels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32dec09a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Apple\n",
       "1    Orange\n",
       "4     Money\n",
       "dtype: object"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money'])\n",
    "ser[ser.str.count(pat=r'[AaEeIiOoUu]') >=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "efb9de7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "Orange\n",
      "Money\n"
     ]
    }
   ],
   "source": [
    "ser = pd.Series(['Apple', 'Orange', 'Plan', 'Python', 'Money'])\n",
    "for i in ser:\n",
    "    a=0\n",
    "    for j in i:\n",
    "        if j in ['a','e','i','o','u','A','E','I','O','U']:\n",
    "            a=a+1\n",
    "            if a>=2:\n",
    "                print(i)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44177063",
   "metadata": {},
   "source": [
    "25) Extract the valid emails from the series emails. The regex pattern for valid emails is provided as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3e85fe14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     []\n",
       "1    [rameses@egypt.com]\n",
       "2            [matt@t.co]\n",
       "3    [narendra@modi.com]\n",
       "dtype: object"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "emails = pd.Series(['buying books at amazom.com', 'rameses@egypt.com', 'matt@t.co', 'narendra@modi.com'])\n",
    "pattern ='[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,4}'\n",
    "#emails.str.findall(pattern,flags=re.ignorecase)\n",
    "emails.str.findall(pattern, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fadf17",
   "metadata": {},
   "source": [
    "26) Compute the mean of weights of each fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "75855cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "apple     6.000000\n",
       "banana    4.000000\n",
       "carrot    6.333333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fruit = pd.Series(np.random.choice(['apple', 'banana', 'carrot'], 10))\n",
    "#print(fruit)\n",
    "weights = pd.Series(np.linspace(1, 10, 10))\n",
    "wei_list=weights.tolist()\n",
    "fru_list=fruit.tolist()\n",
    "weights.groupby(fruit).mean()\n",
    "#examples\n",
    "\n",
    "#> [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]\n",
    "#> ['banana', 'carrot', 'apple', 'carrot', 'carrot', 'apple', 'banana', 'carrot', 'apple', 'carrot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c8ebe7",
   "metadata": {},
   "source": [
    "27) Compute the euclidean distance between series (points) p and q, without using a packaged formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0c9cbfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81, 49, 25, 9, 1, 1, 9, 25, 49, 81]\n",
      "18.16590212458495\n"
     ]
    }
   ],
   "source": [
    "p = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "q = pd.Series([10, 9, 8, 7, 6, 5, 4, 3, 2, 1])\n",
    "  #[d(x, y) = \\sqrt{\\sum_{i=0}^{n}(x_{i}-y_{i})^{2}}\\] \n",
    "dist = np.sqrt(np.sum([(a-b)*(a-b) for a, b in zip(p,q)]))\n",
    "print(([(a-b)*(a-b) for a, b in zip(p,q)]))\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8326a3",
   "metadata": {},
   "source": [
    "28) Get the positions of peaks (values surrounded by smaller values on both sides) in ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3ea30aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 7], dtype=int64)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series([2, 10, 3, 4, 9, 10, 2, 7, 3])\n",
    "dd = np.diff(np.sign(np.diff(ser)))\n",
    "peak_locs = np.where(dd == -2)[0] + 1\n",
    "peak_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5675f",
   "metadata": {},
   "source": [
    "29) Replace the spaces in my_str with the least frequent character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1d4aff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2     2\n",
      "10    2\n",
      "3     2\n",
      "4     1\n",
      "9     1\n",
      "7     1\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[122], line 11\u001b[0m\n\u001b[0;32m      6\u001b[0m current_freq \u001b[39m=\u001b[39m freq\u001b[39m.\u001b[39mdropna()\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[39m# function element_freq.dropna()\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# will  Return a new Series with\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m# missing values removed\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m result \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(ser\u001b[39m.\u001b[39;49mreplace(\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m,current_freq))\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(result)\n\u001b[0;32m     15\u001b[0m \u001b[39m#print(my_ser.value_counts().dropna().index[:-1])\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m#my_str.replace(' ',(my_ser[my_ser.value_counts()[:-1]]))\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m#\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "my_str = 'dbc deb abed gade'\n",
    "my_ser=pd.Series(list(my_str))\n",
    "freq = ser.value_counts()\n",
    "print(freq)\n",
    "\n",
    "current_freq = freq.dropna().index[-1]\n",
    " \n",
    "# function element_freq.dropna()\n",
    "# will  Return a new Series with\n",
    "# missing values removed\n",
    "result = \"\".join(ser.replace(' ',current_freq))\n",
    " \n",
    "print(result)\n",
    "\n",
    "#print(my_ser.value_counts().dropna().index[:-1])\n",
    "#my_str.replace(' ',(my_ser[my_ser.value_counts()[:-1]]))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8beb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original series:\n",
      "abc def abcdef icd\n",
      "c    3\n",
      "     3\n",
      "d    3\n",
      "a    2\n",
      "b    2\n",
      "e    2\n",
      "f    2\n",
      "i    1\n",
      "dtype: int64\n",
      "abcidefiabcdefiicd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "str1 = 'abc def abcdef icd'\n",
    "print(\"Original series:\")\n",
    "print(str1)\n",
    "ser = pd.Series(list(str1))\n",
    "element_freq = ser.value_counts()\n",
    "print(element_freq)\n",
    "current_freq = element_freq.dropna().index[-1]\n",
    "result = \"\".join(ser.replace(' ', current_freq))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b95f0",
   "metadata": {},
   "source": [
    "30) Create a timeseries starting at ‘2000-01-01’ and 10 weekends (saturdays) after that having random numbers as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd8dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2000-01-01', '2000-01-08', '2000-01-15', '2000-01-22',\n",
      "               '2000-01-29', '2000-02-05', '2000-02-12', '2000-02-19',\n",
      "               '2000-02-26', '2000-03-04'],\n",
      "              dtype='datetime64[ns]', freq='W-SAT')\n"
     ]
    }
   ],
   "source": [
    "date=pd.date_range('2000-01-01', periods=10,freq='W-SAT')\n",
    "print(date)\n",
    "#dates = pd.date_range('2000-01-01', periods=10, freq='W-SAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0c3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8a17176",
   "metadata": {},
   "source": [
    "31) Series ser has missing dates and values. Make all missing dates appear and fill up with value from previous date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a10d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000-01-01     1.0\n",
      "2000-01-03    10.0\n",
      "2000-01-06     3.0\n",
      "2000-01-08     NaN\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2000-01-01     1.0\n",
       "2000-01-02     1.0\n",
       "2000-01-03    10.0\n",
       "2000-01-04    10.0\n",
       "2000-01-05    10.0\n",
       "2000-01-06     3.0\n",
       "2000-01-07     3.0\n",
       "2000-01-08     NaN\n",
       "Freq: D, dtype: float64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series([1,10,3,np.nan], index=pd.to_datetime(['2000-01-01', '2000-01-03', '2000-01-06', '2000-01-08']))\n",
    "print(ser)\n",
    "ser.resample('D').ffill() \n",
    "#> 2000-01-01     1.0\n",
    "#> 2000-01-03    10.0\n",
    "#> 2000-01-06     3.0\n",
    "#> 2000-01-08     NaN\n",
    "#> dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b261ac",
   "metadata": {},
   "source": [
    "32) Compute autocorrelations for the first 10 lags of ser. Find out which lag has the largest correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052982b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     25.454983\n",
      "1      9.241528\n",
      "2      0.469953\n",
      "3     -1.439576\n",
      "4     -0.120254\n",
      "5     11.171991\n",
      "6     23.668180\n",
      "7     14.255801\n",
      "8      8.148017\n",
      "9      8.458112\n",
      "10    12.033170\n",
      "11    20.162070\n",
      "12    13.861894\n",
      "13    29.292753\n",
      "14    31.745679\n",
      "15    28.811118\n",
      "16    13.094551\n",
      "17    10.700690\n",
      "18    19.050170\n",
      "19     6.523083\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 0.55, 0.12, -0.13, -0.22, -0.1, 0.03, 0.29, 0.11, -0.13, -0.53]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ser = pd.Series(np.arange(20) + np.random.normal(1, 10, 20))\n",
    "print(ser)\n",
    "\n",
    "autocorrelations = [ser.autocorr(i).round(2) for i in range(11)]\n",
    "autocorrelations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae48ebc",
   "metadata": {},
   "source": [
    "33) Import every 50th row of BostonHousing dataset as a dataframe.\n",
    "    https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09888bc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcsv\u001b[39;00m          \n\u001b[1;32m----> 2\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mhttps://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m     reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(f)\n\u001b[0;32m      4\u001b[0m     out \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv'"
     ]
    }
   ],
   "source": [
    "import csv          \n",
    "with open('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    out = []\n",
    "    for i, row in enumerate(reader):\n",
    "        if i%50 == 0:\n",
    "            out.append(row)\n",
    "\n",
    "df2 = pd.DataFrame(out[1:], columns=out[0])\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d632c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.parsers.readers.TextFileReader'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n",
      "C:\\Users\\ngupt\\AppData\\Local\\Temp\\ipykernel_27688\\621248396.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df2 = df2.append(chunk.iloc[0,:])\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv', chunksize=50)\n",
    "print(type(df))\n",
    "df2 = pd.DataFrame()\n",
    "print(type(df2))\n",
    "for chunk in df:\n",
    "    df2 = df2.append(chunk.iloc[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10cbec",
   "metadata": {},
   "source": [
    "34) Import the boston housing dataset, but while importing change the 'medv' (median house value) column so that values < 25 becomes ‘Low’ and > 25 becomes ‘High’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c39256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a83078db",
   "metadata": {},
   "source": [
    "35) Create a dataframe with rows as strides from the series \"L\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dfeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      2\n",
       "3      3\n",
       "4      4\n",
       "5      5\n",
       "6      6\n",
       "7      7\n",
       "8      8\n",
       "9      9\n",
       "10    10\n",
       "11    11\n",
       "12    12\n",
       "13    13\n",
       "14    14\n",
       "dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = pd.Series(range(15))\n",
    "df=pd.DataFrame(L)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b36f890",
   "metadata": {},
   "source": [
    "36) Import ‘crim’ and ‘medv’ columns of the BostonHousing dataset as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ecddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "      crim  medv\n",
      "0  0.00632  24.0\n",
      "1  0.02731  21.6\n",
      "2  0.02729  34.7\n",
      "3  0.03237  33.4\n",
      "4  0.06905  36.2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv', usecols=['crim', 'medv'])\n",
    "print(type(df))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d830c591",
   "metadata": {},
   "source": [
    "37) Get the number of rows, columns, datatype and summary statistics of each column of the Cars93 dataset. Also get the numpy array and list equivalent of the dataframe.\n",
    "\n",
    "https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1fe83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manufacturer           object\n",
      "Model                  object\n",
      "Type                   object\n",
      "Min.Price             float64\n",
      "Price                 float64\n",
      "Max.Price             float64\n",
      "MPG.city              float64\n",
      "MPG.highway           float64\n",
      "AirBags                object\n",
      "DriveTrain             object\n",
      "Cylinders              object\n",
      "EngineSize            float64\n",
      "Horsepower            float64\n",
      "RPM                   float64\n",
      "Rev.per.mile          float64\n",
      "Man.trans.avail        object\n",
      "Fuel.tank.capacity    float64\n",
      "Passengers            float64\n",
      "Length                float64\n",
      "Wheelbase             float64\n",
      "Width                 float64\n",
      "Turn.circle           float64\n",
      "Rear.seat.room        float64\n",
      "Luggage.room          float64\n",
      "Weight                float64\n",
      "Origin                 object\n",
      "Make                   object\n",
      "dtype: object\n",
      "(93, 27)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "print(df.dtypes)\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e458e997",
   "metadata": {},
   "source": [
    "38) Which manufacturer, model and type has the highest Price? What is the row and column number of the cell with the highest Price value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70df19d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['type', 'model']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[170], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mhttps://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,usecols\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mManufacturer\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(df)\n\u001b[0;32m      3\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mMin.Price\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmin()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1753\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1753\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n\u001b[0;32m   1754\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1755\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:135\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_names \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols_dtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstring\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mset\u001b[39m(usecols)\u001b[39m.\u001b[39missubset(\n\u001b[0;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morig_names\n\u001b[0;32m    134\u001b[0m ):\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_usecols_names(usecols, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49morig_names)\n\u001b[0;32m    137\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames) \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(usecols):  \u001b[39m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:917\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    915\u001b[0m missing \u001b[39m=\u001b[39m [c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m usecols \u001b[39mif\u001b[39;00m c \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m names]\n\u001b[0;32m    916\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 917\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    919\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmissing\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    920\u001b[0m     )\n\u001b[0;32m    922\u001b[0m \u001b[39mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['type', 'model']"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv',usecols=['Manufacturer','model','type'])\n",
    "print(df)\n",
    "df['Min.Price'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e81856",
   "metadata": {},
   "source": [
    "39) Rename the column \"Type\" as \"CarType\" in the previous data and replace the ‘.’ in column names with ‘_’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e517e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "\n",
    "df=df.rename(columns = {'Type':'CarType'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5989022c",
   "metadata": {},
   "source": [
    "40) Check if the data from #37 has any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f7e0ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed881438",
   "metadata": {},
   "source": [
    "41) Count the number of missing values in each column of the previous data. Which column has the maximum number of missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf14525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "n_missings_each_col = df.apply(lambda x: x.isnull().sum())\n",
    "n_missings_each_col.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5abbc84",
   "metadata": {},
   "source": [
    "42) Replace missing values in Min.Price and Max.Price columns with their respective mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91acfe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Min.Price  Max.Price\n",
      "0  12.900000  18.800000\n",
      "1  29.200000  38.700000\n",
      "2  25.900000  32.300000\n",
      "3  17.118605  44.600000\n",
      "4  17.118605  21.459091\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "# Solution\n",
    "df_out = df[['Min.Price', 'Max.Price']] = df[['Min.Price', 'Max.Price']].apply(lambda x: x.fillna(x.mean()))\n",
    "print(df_out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d26407",
   "metadata": {},
   "source": [
    "43) In the previous data, use apply method to replace the missing values in Min.Price with the column’s mean and those in Max.Price with the column’s median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb6209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "# Solution\n",
    "d = {'Min.Price': np.nanmean, 'Max.Price': np.nanmedian}\n",
    "df[['Min.Price', 'Max.Price']] = df[['Min.Price', 'Max.Price']].apply(lambda x, d: x.fillna(d[x.name](x)), args=(d, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c193bc",
   "metadata": {},
   "source": [
    "44) Get the first column (a) in the data as a dataframe (rather than as a Series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2fbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\n",
    "\n",
    "# Solution\n",
    "type(df[['a']])\n",
    "type(df.loc[:, ['a']])\n",
    "type(df.iloc[:, [0]])\n",
    "\n",
    "# Alternately the following returns a Series\n",
    "type(df.a)\n",
    "type(df['a'])\n",
    "type(df.loc[:, 'a'])\n",
    "type(df.iloc[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83334c4f",
   "metadata": {},
   "source": [
    "45) Actually 3 questions.\n",
    "\n",
    "In the data, interchange columns 'a' and 'c'.\n",
    "\n",
    "Create a generic function to interchange two columns, without hardcoding column names.\n",
    "\n",
    "Sort the columns in reverse alphabetical order, that is colume 'e' first through column 'a' last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706e8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c</th>\n",
       "      <th>b</th>\n",
       "      <th>a</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    c   b   a   d   e\n",
       "0   2   1   0   3   4\n",
       "1   7   6   5   8   9\n",
       "2  12  11  10  13  14\n",
       "3  17  16  15  18  19"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.arange(20).reshape(-1, 5), columns=list('abcde'))\n",
    "\n",
    "df[list('cbade')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd957988",
   "metadata": {},
   "source": [
    "46) Change the pandas display settings on printing the dataframe so it shows a maximum of 10 rows and 10 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "# df\n",
    "\n",
    "# Show all available options\n",
    "# pd.describe_option()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a323b",
   "metadata": {},
   "source": [
    "47) Suppress scientific notations like ‘e-03’ in df and print upto 4 numbers after decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485aa0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   random\n",
       "0  0.0000\n",
       "1  0.0600\n",
       "2  0.0000\n",
       "3  0.0249"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.random(4)**10, columns=['random'])\n",
    "\n",
    "df.round(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa22aec",
   "metadata": {},
   "source": [
    "48) Format the values in column 'random' of df as percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d4f4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e38ec\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e38ec_level0_col0\" class=\"col_heading level0 col0\" >random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e38ec_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e38ec_row0_col0\" class=\"data row0 col0\" >64.12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e38ec_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e38ec_row1_col0\" class=\"data row1 col0\" >24.86%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e38ec_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e38ec_row2_col0\" class=\"data row2 col0\" >6.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e38ec_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e38ec_row3_col0\" class=\"data row3 col0\" >7.47%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x181d44784c0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.random(4), columns=['random'])\n",
    "\n",
    "out = df.style.format({\n",
    "    'random': '{0:.2%}'.format,\n",
    "})\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829030ce",
   "metadata": {},
   "source": [
    "49) From the cars data, filter the 'Manufacturer', 'Model' and 'Type' for every 20th row starting from 1st (row 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b439593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Manufacturer    Model     Type\n",
      "0         Acura  Integra    Small\n",
      "20     Chrysler  LeBaron  Compact\n",
      "40        Honda  Prelude   Sporty\n",
      "60      Mercury   Cougar  Midsize\n",
      "80       Subaru   Loyale    Small\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv')\n",
    "\n",
    "print(df.iloc[::20, :][['Manufacturer', 'Model', 'Type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38155b7",
   "metadata": {},
   "source": [
    "50) In the cars data, Replace NaNs with ‘missing’ in columns 'Manufacturer', 'Model' and 'Type' and create a index as a combination of these three columns and check if the index is a primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf131e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/selva86/datasets/master/Cars93_miss.csv', usecols=[0,1,2,3,5])\n",
    "\n",
    "df[['Manufacturer', 'Model', 'Type']] = df[['Manufacturer', 'Model', 'Type']].fillna('missing')\n",
    "df.index = df.Manufacturer + '_' + df.Model + '_' + df.Type\n",
    "print(df.index.is_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01c9b73",
   "metadata": {},
   "source": [
    "51) Find the row position of the 5th largest value of column 'a' in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15321302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randint(1, 30, 30).reshape(10,-1), columns=list('abc'))\n",
    "\n",
    "# Solution\n",
    "n = 5\n",
    "df['a'].argsort()[::-1][n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df76d19",
   "metadata": {},
   "source": [
    "52) In ser, find the position of the 2nd largest value greater than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61330d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ser:  [49, 47, 79, 42, 32, 34, 14, 88, 52, 63, 64, 31, 78, 16, 34] mean:  48\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (1) does not match length of index (15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Solution\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mser: \u001b[39m\u001b[39m'\u001b[39m, ser\u001b[39m.\u001b[39mtolist(), \u001b[39m'\u001b[39m\u001b[39mmean: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39mround\u001b[39m(ser\u001b[39m.\u001b[39mmean()))\n\u001b[1;32m----> 5\u001b[0m np\u001b[39m.\u001b[39;49margwhere(ser \u001b[39m>\u001b[39;49m ser\u001b[39m.\u001b[39;49mmean())[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36margwhere\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\core\\numeric.py:625\u001b[0m, in \u001b[0;36margwhere\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[39m# then remove the added dimension\u001b[39;00m\n\u001b[0;32m    624\u001b[0m     \u001b[39mreturn\u001b[39;00m argwhere(a)[:,:\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 625\u001b[0m \u001b[39mreturn\u001b[39;00m transpose(nonzero(a))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mnonzero\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:1984\u001b[0m, in \u001b[0;36mnonzero\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1892\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_nonzero_dispatcher)\n\u001b[0;32m   1893\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnonzero\u001b[39m(a):\n\u001b[0;32m   1894\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1895\u001b[0m \u001b[39m    Return the indices of the elements that are non-zero.\u001b[39;00m\n\u001b[0;32m   1896\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \n\u001b[0;32m   1983\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1984\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mnonzero\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:47\u001b[0m, in \u001b[0;36m_wrapit\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m     46\u001b[0m         result \u001b[39m=\u001b[39m asarray(result)\n\u001b[1;32m---> 47\u001b[0m     result \u001b[39m=\u001b[39m wrap(result)\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:2107\u001b[0m, in \u001b[0;36mNDFrame.__array_wrap__\u001b[1;34m(self, result, context)\u001b[0m\n\u001b[0;32m   2105\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m   2106\u001b[0m d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_axes_dict(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_AXIS_ORDERS, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 2107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(res, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39md)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__array_wrap__\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\series.py:461\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    459\u001b[0m     index \u001b[39m=\u001b[39m default_index(\u001b[39mlen\u001b[39m(data))\n\u001b[0;32m    460\u001b[0m \u001b[39melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 461\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(data, index)\n\u001b[0;32m    463\u001b[0m \u001b[39m# create/copy the manager\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (1) does not match length of index (15)"
     ]
    }
   ],
   "source": [
    "ser = pd.Series(np.random.randint(1, 100, 15))\n",
    "\n",
    "# Solution\n",
    "print('ser: ', ser.tolist(), 'mean: ', round(ser.mean()))\n",
    "np.argwhere(ser > ser.mean())[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaae1b8",
   "metadata": {},
   "source": [
    "53) Get the last two rows of df whose row sum is greater than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab30612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(10, 40, 60).reshape(-1, 4))\n",
    "\n",
    "\n",
    "rowsums = df.apply(np.sum, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4dac7e",
   "metadata": {},
   "source": [
    "54) Replace all values of ser in the lower 5%ile and greater than 95%ile with respective 5th and 95th %ile value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc91ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 %ile:  0.016049294076965887 | 0.95 %ile:  63.87667222018393\n"
     ]
    }
   ],
   "source": [
    "ser = pd.Series(np.logspace(-2, 2, 30))\n",
    "\n",
    "# Solution\n",
    "def cap_outliers(ser, low_perc, high_perc):\n",
    "    low, high = ser.quantile([low_perc, high_perc])\n",
    "    print(low_perc, '%ile: ', low, '|', high_perc, '%ile: ', high)\n",
    "    ser[ser < low] = low\n",
    "    ser[ser > high] = high\n",
    "    return(ser)\n",
    "\n",
    "capped_ser = cap_outliers(ser, .05, .95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407723d9",
   "metadata": {},
   "source": [
    "55) Reshape df to the largest possible square with negative values removed. Drop the smallest values if need be. The order of the positive numbers in the result should remain the same as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebc8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(-20, 50, 100).reshape(10,-1))\n",
    "arr = df[df > 0].values.flatten()\n",
    "arr_qualified = arr[~np.isnan(arr)]\n",
    "\n",
    "n = int(np.floor(arr_qualified.shape[0]**.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa891f3",
   "metadata": {},
   "source": [
    "56) Swap rows 1 and 2 in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca60052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(25).reshape(5, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97144928",
   "metadata": {},
   "source": [
    "57) Reverse all the rows of dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(25).reshape(5, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f09f84f",
   "metadata": {},
   "source": [
    "58) Get one-hot encodings for column 'a' in the dataframe df and append it as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59860b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(25).reshape(5,-1), columns=list('abcde'))\n",
    "    a   b   c   d   e\n",
    "0   0   1   2   3   4\n",
    "1   5   6   7   8   9\n",
    "2  10  11  12  13  14\n",
    "3  15  16  17  18  19\n",
    "4  20  21  22  23  24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5295b8b",
   "metadata": {},
   "source": [
    "59) Obtain the column name with the highest number of row-wise maximum’s in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5083160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column with highest row maxes:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input\n",
    "df = pd.DataFrame(np.random.randint(1,100, 40).reshape(10, -1))\n",
    "\n",
    "# Solution\n",
    "print('Column with highest row maxes: ', df.apply(np.argmax, axis=1).value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcf24b",
   "metadata": {},
   "source": [
    "60) Create a new column such that, each row contains the row number of nearest row-record by euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0719c737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 40).reshape(10, -1), columns=list('pqrs'), index=list('abcdefghij'))\n",
    "df\n",
    "#     p   q   r   s\n",
    "# a  57  77  13  62\n",
    "# b  68   5  92  24\n",
    "# c  74  40  18  37\n",
    "# d  80  17  39  60\n",
    "# e  93  48  85  33\n",
    "# f  69  55   8  11\n",
    "# g  39  23  88  53\n",
    "# h  63  28  25  61\n",
    "# i  18   4  73   7\n",
    "# j  79  12  45  34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eb1791",
   "metadata": {},
   "source": [
    "61) Compute maximum possible absolute correlation value of each column against other columns in df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee6e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Correlation possible for each column:  [0.72 0.58 0.9  0.6  0.63 0.63 0.67 0.67 0.49 0.9 ]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1), columns=list('pqrstuvwxy'), index=list('abcdefgh'))\n",
    "abs_corrmat = np.abs(df.corr())\n",
    "max_corr = abs_corrmat.apply(lambda x: sorted(x)[-2])\n",
    "print('Maximum Correlation possible for each column: ', np.round(max_corr.tolist(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ef471",
   "metadata": {},
   "source": [
    "62) Compute the minimum-by-maximum for every row of df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7368340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))\n",
    "\n",
    "min_by_max = df.apply(lambda x: np.min(x)/np.max(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d2276",
   "metadata": {},
   "source": [
    "63) Create a new column 'penultimate' which has the second largest value of each row of df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529c1042",
   "metadata": {},
   "source": [
    "64) Normalize all columns of df by subtracting the column mean and divide by standard deviation.\n",
    "Range all columns of df such that the minimum value in each column is 0 and max is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da28502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution Q1\n",
      "       0     1     2     3     4     5     6     7     8     9\n",
      "0  0.79  0.91 -0.88  1.13  0.53  0.25  0.59 -0.07  0.79 -1.78\n",
      "1  0.62  0.79  1.43 -0.72 -1.31  0.97  0.88  1.40  1.50  1.06\n",
      "2 -1.21  0.83 -0.94 -0.82 -1.35 -0.28 -0.74  0.61  0.83 -0.00\n",
      "3  0.51 -0.98  1.28  0.74  0.69 -0.52 -1.52 -0.22 -0.39 -0.71\n",
      "4  0.31  0.31 -0.34 -0.86  0.26  1.45 -0.23 -0.22  0.07  0.71\n",
      "5 -0.87  0.47  0.14 -1.25 -0.68  0.37 -0.93 -1.44 -0.26  0.88\n",
      "6 -1.38 -0.50  0.46  1.13  1.40 -1.80  0.66 -1.15 -1.35  0.60\n",
      "7  1.22 -1.83 -1.14  0.64  0.46 -0.44  1.29  1.08 -1.18 -0.75\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))\n",
    "out1 = df.apply(lambda x: ((x - x.mean())/x.std()).round(2))\n",
    "print('Solution Q1\\n',out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af26e808",
   "metadata": {},
   "source": [
    "65) Compute the correlation of each row of df with its succeeding row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fca730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04, -0.51, -0.26, 0.31, 0.43, 0.06, 0.54]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 80).reshape(8, -1))\n",
    "[df.iloc[i].corr(df.iloc[i+1]).round(2) for i in range(df.shape[0])[:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57341a09",
   "metadata": {},
   "source": [
    "66) Replace both values in both diagonals of df with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721432ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randint(1,100, 100).reshape(10, -1))\n",
    "df\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    df.iat[i, i] = 0\n",
    "    df.iat[df.shape[0]-i-1, i] = 0\n",
    "#     0   1   2   3   4   5   6   7   8   9\n",
    "# 0  11  46  26  44  11  62  18  70  68  26\n",
    "# 1  87  71  52  50  81  43  83  39   3  59\n",
    "# 2  47  76  93  77  73   2   2  16  14  26\n",
    "# 3  64  18  74  22  16  37  60   8  66  39\n",
    "# 4  10  18  39  98  25   8  32   6   3  29\n",
    "# 5  29  91  27  86  23  84  28  31  97  10\n",
    "# 6  37  71  70  65   4  72  82  89  12  97\n",
    "# 7  65  22  97  75  17  10  43  78  12  77\n",
    "# 8  47  57  96  55  17  83  61  85  26  86\n",
    "# 9  76  80  28  45  77  12  67  80   7  63\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c483b",
   "metadata": {},
   "source": [
    "67) Using a key in a grouped data frame. From df_grouped, get the group belonging to 'apple' as a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8028ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'col1': ['apple', 'banana', 'orange'] * 3,\n",
    "                   'col2': np.random.rand(9),\n",
    "                   'col3': np.random.randint(0, 15, 9)})\n",
    "\n",
    "df_grouped = df.groupby(['col1'])\n",
    "for i, dff in df_grouped:\n",
    "    if i == 'apple':\n",
    "        print(dff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2cb901",
   "metadata": {},
   "source": [
    "68) In df, find the second largest value of 'taste' for 'banana'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7209bc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n",
    "                   'rating': np.random.rand(9),\n",
    "                   'price': np.random.randint(0, 15, 9)})\n",
    "df_grpd = df['taste'].groupby(df.fruit)\n",
    "df_grpd.get_group('banana').sort_values().iloc[-2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bde89cb",
   "metadata": {},
   "source": [
    "69) In df, Compute the mean price of every fruit, while keeping the fruit as another column instead of an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "69a4c8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fruit     price\n",
      "0   apple  4.333333\n",
      "1  banana  8.333333\n",
      "2  orange  8.666667\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n",
    "                   'rating': np.random.rand(9),\n",
    "                   'price': np.random.randint(0, 15, 9)})\n",
    "out = df.groupby('fruit', as_index=False)['price'].mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eaa372",
   "metadata": {},
   "source": [
    "70) Join dataframes df1 and df2 by ‘fruit-pazham’ and ‘weight-kilo’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a250adf",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[125], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m df1 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mfruit\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mapple\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbanana\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morange\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[0;32m      2\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmedium\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[0;32m      3\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m9\u001b[39m)})\n\u001b[0;32m      5\u001b[0m df2 \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mpazham\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mapple\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morange\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpine\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[0;32m      6\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mkilo\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mhigh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlow\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m,\n\u001b[0;32m      7\u001b[0m                     \u001b[39m'\u001b[39m\u001b[39mprice\u001b[39m\u001b[39m'\u001b[39m: np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m6\u001b[39m)})\n\u001b[1;32m----> 8\u001b[0m pd\u001b[39m.\u001b[39;49mmerge(df1, df2, how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39minner\u001b[39;49m\u001b[39m'\u001b[39;49m, left_on\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mfruit\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mweight\u001b[39;49m\u001b[39m'\u001b[39;49m], right_on\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mpazham\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpounds\u001b[39;49m\u001b[39m'\u001b[39;49m], suffixes\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m_left\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m_right\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    108\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    109\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m--> 110\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[0;32m    111\u001b[0m         left,\n\u001b[0;32m    112\u001b[0m         right,\n\u001b[0;32m    113\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[0;32m    114\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[0;32m    115\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[0;32m    116\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[0;32m    117\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[0;32m    118\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[0;32m    119\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    120\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[0;32m    121\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[0;32m    122\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    123\u001b[0m     )\n\u001b[0;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result(copy\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:703\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cross \u001b[39m=\u001b[39m cross_col\n\u001b[0;32m    698\u001b[0m \u001b[39m# note this function has side effects\u001b[39;00m\n\u001b[0;32m    699\u001b[0m (\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[0;32m    701\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[0;32m    702\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[1;32m--> 703\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_merge_keys()\n\u001b[0;32m    705\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1162\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1160\u001b[0m rk \u001b[39m=\u001b[39m cast(Hashable, rk)\n\u001b[0;32m   1161\u001b[0m \u001b[39mif\u001b[39;00m rk \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1162\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39;49m_get_label_or_level_values(rk))\n\u001b[0;32m   1163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1164\u001b[0m     \u001b[39m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:1850\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1844\u001b[0m     values \u001b[39m=\u001b[39m (\n\u001b[0;32m   1845\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\n\u001b[0;32m   1846\u001b[0m         \u001b[39m.\u001b[39mget_level_values(key)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m         \u001b[39m.\u001b[39m_values\n\u001b[0;32m   1848\u001b[0m     )\n\u001b[0;32m   1849\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1850\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m   1852\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pounds'"
     ]
    }
   ],
   "source": [
    "df1 = pd.DataFrame({'fruit': ['apple', 'banana', 'orange'] * 3,\n",
    "                    'weight': ['high', 'medium', 'low'] * 3,\n",
    "                    'price': np.random.randint(0, 15, 9)})\n",
    "\n",
    "df2 = pd.DataFrame({'pazham': ['apple', 'orange', 'pine'] * 2,\n",
    "                    'kilo': ['high', 'low'] * 3,\n",
    "                    'price': np.random.randint(0, 15, 6)})\n",
    "pd.merge(df1, df2, how='inner', left_on=['fruit', 'weight'], right_on=['pazham', 'pounds'], suffixes=['_left', '_right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57adb8fe",
   "metadata": {},
   "source": [
    "71) Get the positions where the value of two columns match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b43cc070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fruit  weight  price\n",
      "0   apple    high      0\n",
      "1  banana  medium      4\n",
      "2  orange     low      3\n",
      "3   apple    high     13\n",
      "4  banana  medium     14\n",
      "5  orange     low     10\n",
      "6   apple    high     10\n",
      "7  banana  medium      5\n",
      "8  orange     low      2\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'fruit1': np.random.choice(['apple', 'orange', 'banana'], 10),\n",
    "                    'fruit2': np.random.choice(['apple', 'orange', 'banana'], 10)})\n",
    "print(df1[~df1.isin(df2).all(1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da5c86",
   "metadata": {},
   "source": [
    "72) Create two new columns in df, one of which is a lag1 (shift column a down by 1 row) of column ‘a’ and the other is a lead1 (shift column b up by 1 row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e531adc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 7, 8, 9], dtype=int64),)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'fruit1': np.random.choice(['apple', 'orange', 'banana'], 10),\n",
    "                    'fruit2': np.random.choice(['apple', 'orange', 'banana'], 10)})\n",
    "\n",
    "\n",
    "np.where(df.fruit1 == df.fruit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a01f7",
   "metadata": {},
   "source": [
    "73) Get the frequency of unique values in the entire dataframe df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c22b1df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b  c  d  a_lag1  b_lead1\n",
      "0  5  9  9  6     NaN      6.0\n",
      "1  7  6  4  3     5.0      7.0\n",
      "2  1  7  1  8     7.0      9.0\n",
      "3  2  9  5  9     1.0      7.0\n",
      "4  2  7  1  4     2.0      NaN\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(np.random.randint(1, 10, 20).reshape(-1, 4), columns = list('abcd'))\n",
    "df['a_lag1'] = df['a'].shift(1)\n",
    "df['b_lead1'] = df['b'].shift(-1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d182d",
   "metadata": {},
   "source": [
    "74) Split the string column in df to form a dataframe with 3 columns as shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4d8a2bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          row\n",
      "0          STD, City    State\n",
      "1  33, Kolkata    West Bengal\n",
      "2   44, Chennai    Tamil Nadu\n",
      "3  40, Hyderabad    Telengana\n",
      "4  80, Bangalore    Karnataka\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame([\"STD, City    State\",\n",
    "\"33, Kolkata    West Bengal\",\n",
    "\"44, Chennai    Tamil Nadu\",\n",
    "\"40, Hyderabad    Telengana\",\n",
    "\"80, Bangalore    Karnataka\"], columns=['row'])\n",
    "\n",
    "print(df)\n",
    "\n",
    "df_out = df.row.str.split(',|\\t', expand=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
